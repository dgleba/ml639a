{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dg test ir4 chips  (based on casting void - cast03 e03cast.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ds9xP7rtnrZ"
   },
   "source": [
    "Next, change directory to wherever you created your folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkyXFsLGtvyc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Change this to your Drive folder location\n",
    "WORKING_DIRECTORY = '/notebooks/ml639a/639e'\n",
    "imgdir='/notebooks/imgdata/inrm4pyt'\n",
    "imgtrain = '/notebooks/imgdata/inrm4pyt/train'\n",
    "imgval = '/notebooks/imgdata/inrm4pyt/val'\n",
    "\n",
    "os.chdir(WORKING_DIRECTORY)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1654738344918,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "nNbnk4pitjF0",
    "outputId": "a9ad9d39-1104-4098-fda5-f31c570f58be"
   },
   "outputs": [],
   "source": [
    "# List the contents of your working directory\n",
    "# It should contain at least three folders: images, train_labels, and val_labels\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43sPuLgxuemn"
   },
   "source": [
    "Now, let's install the Detecto package using pip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3427,
     "status": "ok",
     "timestamp": 1654738351538,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "M3-sRIuiuYDh",
    "outputId": "b97165ba-2a42-4df2-8c2d-982be162c3fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: if it states you must restart the runtime in order to use a\n",
    "# newly installed version of a package, you do NOT need to do this. \n",
    "!pip install detecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peLysju5vOgA"
   },
   "source": [
    "Import everything we need in the following code block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y python3-opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgJi8407uowH",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import transforms\n",
    "from detecto import core, utils, visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5UqkoIevXSX"
   },
   "source": [
    "To check that everything's working, we can try reading in one of the images from our images folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "find ./images -type f | tail -n1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1654738365701,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "FmjpizSMvJLn",
    "outputId": "d514d7dc-8752-4832-adbd-741915861fc0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = utils.read_image('/notebooks/imgdata/inrm4pyt/train/inner_rim_210805T103940.png')\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCEC_0EQwLDf"
   },
   "source": [
    "How cute! Now, we're ready to create our dataset and train our model. However, before doing so, it's a bit slow working with hundreds of individual XML label files, so we should convert them into a single CSV file to save time later down the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 12135,
     "status": "ok",
     "timestamp": 1654738743456,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "_Pnr8quRv8v7",
    "outputId": "53a5f852-6dbe-4e3f-c754-6ea474f9bb2a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do this twice: once for our trning labels and once for our validation labels\n",
    "utils.xml_to_csv(imgtrain, 'train.csv')\n",
    "utils.xml_to_csv(imgval, 'val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csLF8GKQ3CGe"
   },
   "source": [
    "Below, we create our dataset, applying a couple of transforms beforehand. These are optional, but they can be useful for augmenting your dataset without gathering more data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1654741423315,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "m7j17jxf31Gk",
    "outputId": "2f47347d-66ca-485b-f655-216806d8ae4c",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m dataset \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, imgtrain, transform\u001b[38;5;241m=\u001b[39mtransform_img)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# dataset[i] returns a tuple containing our transformed image and\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# and a dictionary containing label and box data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m image, target \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Show our image along with the box. Note: it may\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# be colored oddly due to being normalized by the \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# dataset and then reverse-normalized for plotting\u001b[39;00m\n\u001b[1;32m     26\u001b[0m visualize\u001b[38;5;241m.\u001b[39mshow_labeled_image(image, target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m], target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/detecto/core.py:212\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    209\u001b[0m         image \u001b[38;5;241m=\u001b[39m t(image)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Scale down box if necessary\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scale_factor \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, box \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(targets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    214\u001b[0m         box \u001b[38;5;241m=\u001b[39m (box \u001b[38;5;241m/\u001b[39m scale_factor)\u001b[38;5;241m.\u001b[39mlong()\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "# Specify a list of transformations for our dataset to apply on our images\n",
    "transform_img00 = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(800),\n",
    "    transforms.RandomHorizontalFlip(0.5),\n",
    "    transforms.ToTensor(),\n",
    "    utils.normalize_transform(),\n",
    "])\n",
    "\n",
    "transform_img = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize([512,256]),\n",
    "    transforms.ToTensor(),\n",
    "    utils.normalize_transform(),\n",
    "])\n",
    "\n",
    "dataset = core.Dataset('train.csv', imgtrain, transform=transform_img)\n",
    "\n",
    "# dataset[i] returns a tuple containing our transformed image and\n",
    "# and a dictionary containing label and box data\n",
    "image, target = dataset[0]\n",
    "\n",
    "# Show our image along with the box. Note: it may\n",
    "# be colored oddly due to being normalized by the \n",
    "# dataset and then reverse-normalized for plotting\n",
    "visualize.show_labeled_image(image, target['boxes'], target['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kPFYk-lv6Cgd"
   },
   "source": [
    "Finally, let's train our model! First, we create a DataLoader over our dataset to specify how we feed the images into our model. We also use our validation dataset to track the accuracy of the model throughout training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 29 20:45:12 UTC 2022\n"
     ]
    }
   ],
   "source": [
    "!date -d \"-4 hours\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 50319,
     "status": "error",
     "timestamp": 1654741490003,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "WLRHugVo6kAt",
    "outputId": "a9c7a102-cfe6-40eb-ed6e-64d5a0e3c02d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 29 20:46:32 UTC 2022\n",
      "Epoch 1 of 3\n",
      "Begin iterating over training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m core\u001b[38;5;241m.\u001b[39mModel([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChip\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model! This step can take a while, so make sure you\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# the GPU is turned on in Edit -> Notebook settings\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Plot the accuracy over time\u001b[39;00m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/detecto/core.py:523\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, dataset, val_dataset, epochs, learning_rate, momentum, weight_decay, gamma, lr_step_size, verbose)\u001b[0m\n\u001b[1;32m    519\u001b[0m images, targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_device(images, targets)\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# Calculate the model's loss (i.e. how well it does on the current\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# image and target, with a lower loss being better)\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# Zero any old/existing gradients on the model's parameters\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:99\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     97\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m     98\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m---> 99\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[1;32m    102\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/detection/roi_heads.py:751\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    748\u001b[0m     regression_targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    749\u001b[0m     matched_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 751\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbox_roi_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    752\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_head(box_features)\n\u001b[1;32m    753\u001b[0m class_logits, box_regression \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_predictor(box_features)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/ops/poolers.py:330\u001b[0m, in \u001b[0;36mMultiScaleRoIAlign.forward\u001b[0;34m(self, x, boxes, image_shapes)\u001b[0m\n\u001b[1;32m    328\u001b[0m x_filtered \u001b[38;5;241m=\u001b[39m _filter_input(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatmap_names)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_levels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 330\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscales, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_levels \u001b[38;5;241m=\u001b[39m \u001b[43m_setup_scales\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonical_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanonical_level\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _multiscale_roi_align(\n\u001b[1;32m    335\u001b[0m     x_filtered,\n\u001b[1;32m    336\u001b[0m     boxes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_levels,\n\u001b[1;32m    341\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/ops/poolers.py:123\u001b[0m, in \u001b[0;36m_setup_scales\u001b[0;34m(features, image_shapes, canonical_scale, canonical_level)\u001b[0m\n\u001b[1;32m    120\u001b[0m     max_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(shape[\u001b[38;5;241m1\u001b[39m], max_y)\n\u001b[1;32m    121\u001b[0m original_input_shape \u001b[38;5;241m=\u001b[39m (max_x, max_y)\n\u001b[0;32m--> 123\u001b[0m scales \u001b[38;5;241m=\u001b[39m [_infer_scale(feat, original_input_shape) \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# get the levels in the feature map by leveraging the fact that the network always\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# downsamples by a factor of 2 at each level.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m lvl_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mtensor(scales[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/ops/poolers.py:123\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    120\u001b[0m     max_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(shape[\u001b[38;5;241m1\u001b[39m], max_y)\n\u001b[1;32m    121\u001b[0m original_input_shape \u001b[38;5;241m=\u001b[39m (max_x, max_y)\n\u001b[0;32m--> 123\u001b[0m scales \u001b[38;5;241m=\u001b[39m [\u001b[43m_infer_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_input_shape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m feat \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# get the levels in the feature map by leveraging the fact that the network always\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m# downsamples by a factor of 2 at each level.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m lvl_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlog2(torch\u001b[38;5;241m.\u001b[39mtensor(scales[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/ops/poolers.py:107\u001b[0m, in \u001b[0;36m_infer_scale\u001b[0;34m(feature, original_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m     scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(torch\u001b[38;5;241m.\u001b[39mtensor(approx_scale)\u001b[38;5;241m.\u001b[39mlog2()\u001b[38;5;241m.\u001b[39mround())\n\u001b[1;32m    106\u001b[0m     possible_scales\u001b[38;5;241m.\u001b[39mappend(scale)\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m possible_scales[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m possible_scales[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m possible_scales[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create our validation dataset\n",
    "!date -d \"-4 hours\"\n",
    "val_dataset = core.Dataset('val.csv', imgval)\n",
    "\n",
    "# Create the loader for our training dataset\n",
    "loader = core.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Create our model, passing in all unique classes we're predicting\n",
    "# Note: make sure these match exactly with the labels in the XML/CSV files!\n",
    "model = core.Model(['Chip'])\n",
    "\n",
    "# Train the model! This step can take a while, so make sure you\n",
    "# the GPU is turned on in Edit -> Notebook settings\n",
    "losses = model.fit(loader, val_dataset, epochs=3, verbose=True)\n",
    "\n",
    "# Plot the accuracy over time\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "!date -d \"-4 hours\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JeRKZMNJ9cfc"
   },
   "source": [
    "Let's see how well our model does on a couple images from our validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 716
    },
    "executionInfo": {
     "elapsed": 3480,
     "status": "ok",
     "timestamp": 1654739642859,
     "user": {
      "displayName": "David Gleba",
      "userId": "15493013878878444265"
     },
     "user_tz": 240
    },
    "id": "rvHbAcLb9cIL",
    "outputId": "b3187aa0-913e-49f5-e767-858387ef12e6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = []\n",
    "# Create a list of images  from val_dataset\n",
    "for i in range(5, 9, 1):\n",
    "    image, _ = val_dataset[i]\n",
    "    images.append(image)\n",
    "    print(image)\n",
    "\n",
    "# Plot a 3x3 grid of the model's predictions on our 9 images\n",
    "visualize.plot_prediction_grid(model, images, dim=(2, 2), figsize=(28, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find ./testfinal -type f | tail -n -3\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=os.listdir(\"./testfinal\")\n",
    "print(f1)\n",
    "import pathlib\n",
    "for filepath in pathlib.Path(\"testfinal/\").glob('**/*'):\n",
    "    print(filepath.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# one image prediction.\n",
    "\n",
    "image = utils.read_image('./testfinal/cast_def_0_9591.jpeg')\n",
    "predictions = model.predict(image)\n",
    "\n",
    "images = []\n",
    "# for i in range(4):\n",
    "#     image, _ = val_dataset[i]\n",
    "images.append(image)\n",
    "\n",
    "top_predictions = model.predict_top(images)\n",
    "\n",
    "print(predictions)\n",
    "print(top_predictions)\n",
    "visualize.plot_prediction_grid(model, images, dim=(1, 1), figsize=(28, 28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# error -  IndexError: index 0 is out of bounds for axis 0 with size 0\n",
    "\n",
    "!pwd\n",
    "testfinal_data = core.Dataset( 'testfinal/')\n",
    "print(val_dataset)\n",
    "print(val_dataset[0])\n",
    "print(testfinal_data)\n",
    "print(testfinal_data[1])\n",
    "images = []\n",
    "for i in range(0,3,1):\n",
    "    print(i)\n",
    "    image, _ = testfinal_data[i]\n",
    "    images.append(image)\n",
    "top_predictions = model.predict_top(images)\n",
    "# print(predictions)\n",
    "print(top_predictions)    \n",
    "visualize.plot_prediction_grid(model, images, dim=(4, 1), figsize=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 203 loop show predictions\n",
    "\n",
    "images = []\n",
    "import pathlib\n",
    "for filepath in pathlib.Path(\"testfinal/\").glob('**/*'):\n",
    "    print(filepath.absolute())\n",
    "    path01=str(filepath.absolute())\n",
    "    image = utils.read_image(path01)\n",
    "    images.append(image)\n",
    "\n",
    "# top_predictions = model.predict_top(images)\n",
    "# print(top_predictions)\n",
    "\n",
    "visualize.plot_prediction_grid(model, images, dim=(4, 1), figsize=(44, 44))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "dtnow = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "fout = f\"../../modelweights_cast03_{dtnow}.pth\"\n",
    "model.save(fout)\n",
    "# model.save(\"modelweight.pth\")\n",
    "!pwd;ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDz1v5Lh-uZG"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Thanks for making it this far through the demo!\n",
    "\n",
    "This is as far as the demo goes, but a great next step would be seeing how well the model works on a live video of Chihuahuas and Golden Retrievers in the same frame at the same time. To learn more about Detecto, be sure to check out the [Quickstart guide](https://detecto.readthedocs.io/en/latest/usage/quickstart.html), [Further Usage guide](https://detecto.readthedocs.io/en/latest/usage/further-usage.html), and [API docs](https://detecto.readthedocs.io/en/latest/api.html)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8PgWhg2B0XhK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "detecto-dg01.ipynb",
   "provenance": [
    {
     "file_id": "1ISaTV5F-7b4i2QqtjTa7ToDPQ2k8qEe0",
     "timestamp": 1654741560702
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
